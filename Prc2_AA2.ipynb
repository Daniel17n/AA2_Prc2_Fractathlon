{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb81b92b",
   "metadata": {},
   "source": [
    "\n",
    "# Clasificación de Fractales con Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Usaremos un modelo basado en CNN con transfer learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1cc7d",
   "metadata": {},
   "source": [
    "\n",
    "## Paso 1: Preparación y análisis del dataset\n",
    "\n",
    "Cargaremos los datos y visualizaremos algunas imágenes de fractales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Añadir el parser de argumentos\n",
    "parser = argparse.ArgumentParser(description='Entrenamiento y predicción de fractales')\n",
    "parser.add_argument('--model-preload', action='store_true', help='Cargar el modelo guardado si existe')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "# Configuración\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 9\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 20  # Para early stopping\n",
    "NUM_EPOCHS = 60\n",
    "GRAD_CLIP = 1.0  # Limitar los gradientes para evitar explosiones\n",
    "\n",
    "class FractalDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.label_map = {label: i for i, label in enumerate(sorted(self.data['fractal'].unique()))}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['relative_path']\n",
    "        label = self.data.iloc[idx]['fractal']\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img.astype(np.float32) / 255.0  # Normalización [0,1]\n",
    "        img = np.expand_dims(img, axis=0)  # Agregar canal\n",
    "        label_idx = self.label_map[label]\n",
    "        \n",
    "        label_idx = self.label_map[label]\n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(label_idx, dtype=torch.long)\n",
    "\n",
    "class FractalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FractalCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 128x128\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(32), nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "            \n",
    "            # 64x64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64), nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "            \n",
    "            # 32x32\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128), nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "            \n",
    "            # 16x16\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(256), nn.MaxPool2d(2), nn.Dropout(0.25)\n",
    "            # 8x8\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv('train.csv')\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = FractalDataset(train_df)\n",
    "val_dataset = FractalDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count())\n",
    "\n",
    "# Configuración del modelo\n",
    "model = FractalCNN().to(DEVICE)\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Intentar cargar el modelo guardado si existe y si se especifica la flag\n",
    "start_epoch = 0\n",
    "best_val_acc = 0.0\n",
    "if args.model_preload and os.path.exists('modelos/mejor_modelo.pth'):\n",
    "    print(\"Cargando modelo guardado...\")\n",
    "    checkpoint = torch.load('modelos/mejor_modelo.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_acc = checkpoint['val_accuracy']\n",
    "    print(f\"Modelo cargado desde la época {start_epoch} con mejor precisión de validación: {best_val_acc:.2f}%\")\n",
    "else:\n",
    "    print(\"Iniciando entrenamiento desde cero.\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=30, patience=5, start_epoch=0, best_val_acc=0.0):\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Crear directorio para guardar el modelo si no existe\n",
    "    os.makedirs('modelos', exist_ok=True)\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluación en validación\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val Acc: {val_accuracy:.2f}% | Best Val Acc: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            patience_counter = 0\n",
    "            # Guardar el mejor modelo\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'modelos/mejor_modelo.pth')\n",
    "            print(f\"¡Nuevo mejor modelo guardado con precisión de validación: {val_accuracy:.2f}%!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience and val_accuracy < 90:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def generar_submission():\n",
    "    # Lista de fractales en orden\n",
    "    FRACTALES = [\n",
    "        'tree',\n",
    "        'koch',\n",
    "        'dragon',\n",
    "        'sierpinski_carpet',\n",
    "        'julia',\n",
    "        'sierpinski',\n",
    "        'newton',\n",
    "        'barnsley',\n",
    "        'mandelbrot'\n",
    "    ]\n",
    "\n",
    "    # Cargar el modelo entrenado\n",
    "    model = FractalCNN().to(DEVICE)\n",
    "    checkpoint = torch.load('modelos/mejor_modelo.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Obtener la lista de imágenes en la carpeta test\n",
    "    test_dir = 'test'  # Asegúrate de que esta es la ruta correcta a tu carpeta de test\n",
    "    imagenes = [f for f in os.listdir(test_dir) if f.endswith('.png')]\n",
    "\n",
    "    # Crear el DataFrame con las rutas relativas\n",
    "    submission_df = pd.DataFrame({\n",
    "        'relative_path': [f'test/{img}' for img in imagenes]\n",
    "    })\n",
    "\n",
    "    # Hacer predicciones\n",
    "    print(\"Iniciando predicciones...\")\n",
    "    predictions = []\n",
    "    for i, img_path in enumerate(submission_df['relative_path']):\n",
    "        if i % 100 == 0:  # Mostrar progreso cada 100 imágenes\n",
    "            print(f\"Procesando imagen {i}/{len(submission_df)}\")\n",
    "        \n",
    "        # Cargar y preprocesar la imagen\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.expand_dims(img, axis=0)  # Añadir canal\n",
    "        img = np.expand_dims(img, axis=0)  # Añadir dimensión de batch\n",
    "        img = torch.tensor(img, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        # Hacer la predicción\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            pred_label = FRACTALES[predicted.item()]\n",
    "            predictions.append(pred_label)\n",
    "\n",
    "    # Actualizar el DataFrame con las predicciones\n",
    "    submission_df['fractal'] = predictions\n",
    "\n",
    "    # Guardar el archivo de submission con el nombre específico\n",
    "    submission_df.to_csv('DragonesYDatos-submission.csv', index=False)\n",
    "    print(\"¡Archivo DragonesYDatos-submission.csv generado exitosamente!\")\n",
    "\n",
    "    # Mostrar las primeras filas del archivo de submission\n",
    "    print(\"\\nPrimeras filas del archivo de submission:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "# Primero entrenar el modelo\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=NUM_EPOCHS, patience=PATIENCE, start_epoch=start_epoch, best_val_acc=best_val_acc)\n",
    "\n",
    "# Luego generar el archivo de submission\n",
    "generar_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbeb497",
   "metadata": {},
   "source": [
    "\n",
    "### Visualización de ejemplos de fractales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "def show_images(df, folder='train', num_images=4):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15,5))\n",
    "    for i in range(num_images):\n",
    "        img_path = os.path.join(folder, df.iloc[i]['relative_path'])\n",
    "        image = Image.open(img_path)\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(df.iloc[i]['fractal'])\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_images(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeabef0",
   "metadata": {},
   "source": [
    "\n",
    "## Paso 2: Preprocesamiento de datos y Data Augmentation\n",
    "\n",
    "Utilizaremos técnicas de aumento de datos para mejorar la robustez del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef37999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.1,\n",
    "                             rotation_range=15, zoom_range=0.1,\n",
    "                             horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='train',\n",
    "    x_col='relative_path',\n",
    "    y_col='fractal',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='train',\n",
    "    x_col='relative_path',\n",
    "    y_col='fractal',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7697dfb",
   "metadata": {},
   "source": [
    "\n",
    "## Paso 3: Definición y entrenamiento del modelo CNN con Transfer Learning\n",
    "\n",
    "Utilizaremos EfficientNetB0 preentrenado en ImageNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(train_df['fractal'].nunique(), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c955a",
   "metadata": {},
   "source": [
    "\n",
    "### Entrenamiento del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97499c69",
   "metadata": {},
   "source": [
    "\n",
    "## Paso 4: Evaluación del modelo\n",
    "\n",
    "Visualizaremos la precisión del modelo durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e60419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
